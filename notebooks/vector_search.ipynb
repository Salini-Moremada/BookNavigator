{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-25T09:41:07.192658Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # updated import\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "\n",
    "books = pd.read_csv(r\"C:\\Users\\MSI\\Desktop\\Book Sphere\\data\\books_cleaned.csv\")\n",
    "print(books.head())\n",
    "\n",
    "with open(\"tagged_description.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in books[\"tagged_description\"].tolist():\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load document\n",
    "raw_documents = TextLoader(\"tagged_description.txt\", encoding=\"utf-8\").load()\n",
    "\n",
    "# Manually split by newline (like chunk_size=0)\n",
    "documents = []\n",
    "for doc in raw_documents:\n",
    "    lines = doc.page_content.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip():  # skip empty lines\n",
    "            documents.append(Document(page_content=line))\n",
    "\n",
    "print(f\"Total chunks created: {len(documents)}\")\n",
    "\n",
    "documents[0]\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Example usage with Chroma\n",
    "texts = [\"This is a book about AI.\", \"This is a novel about love.\"]\n",
    "db = Chroma.from_texts(texts, embeddings, persist_directory=\"db_books\")\n",
    "\n",
    "# Test search\n",
    "query = \"romantic stories\"\n",
    "results = db.similarity_search(query, k=1)\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n",
    "!pip install -U ipywidgets\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Example usage with Chroma\n",
    "texts = [\"This is a book about AI.\", \"This is a novel about love.\"]\n",
    "db = Chroma.from_texts(texts, embeddings, persist_directory=\"db_books\")\n",
    "\n",
    "# Test search\n",
    "query = \"romantic stories\"\n",
    "results = db.similarity_search(query, k=10)\n",
    "print(results)\n",
    "\n",
    "query = \"romantic stories\"\n",
    "results = db.similarity_search(query, k=1)  # or k=10\n",
    "\n",
    "# The first document\n",
    "first_doc = results[0]\n",
    "\n",
    "print(first_doc.page_content)\n",
    "\n",
    "isbn_str = first_doc.page_content.split(\"\\n\")[0].strip()\n",
    "\n",
    "try:\n",
    "    isbn_num = int(isbn_str)\n",
    "except ValueError:\n",
    "    print(f\"Cannot convert ISBN to int: {isbn_str}\")\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Create documents with ISBNs in metadata\n",
    "documents = []\n",
    "for idx, row in books.iterrows():\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content=row['tagged_description'],\n",
    "            metadata={\"isbn13\": row['isbn13']}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Build Chroma DB from these documents\n",
    "db = Chroma.from_documents(documents, embeddings, persist_directory=\"db_books\")\n",
    "\n",
    "# Example search\n",
    "query = \"romantic stories\"\n",
    "results = db.similarity_search(query, k=1)\n",
    "\n",
    "# Get ISBN from metadata\n",
    "first_doc = results[0]\n",
    "isbn_num = first_doc.metadata[\"isbn13\"]\n",
    "print(\"Matched ISBN:\", isbn_num)\n",
    "\n",
    "# Get book info from DataFrame\n",
    "matched_books = books[books[\"isbn13\"] == isbn_num]\n",
    "print(matched_books)\n",
    "\n",
    "# Example: search query\n",
    "query = \"books for babies about animals\"\n",
    "docs = db.similarity_search(query, k=10)\n",
    "\n",
    "# Suppose the ISBN is the first number in the document content\n",
    "doc_content = docs[0].page_content.strip()\n",
    "\n",
    "# Remove extra spaces and split if needed\n",
    "isbn_num = ''.join(filter(str.isdigit, doc_content))  # keep only digits\n",
    "isbn_num = int(isbn_num)\n",
    "\n",
    "matched_books = books[books[\"isbn13\"] == isbn_num]\n",
    "\n",
    "print(matched_books)\n",
    "\n",
    "doc_content = docs[0].page_content.strip()\n",
    "isbn_str = doc_content.split()[0]  # take first word if ISBN is at start\n",
    "isbn_num = int(''.join(filter(str.isdigit, isbn_str)))  # keep only digits\n",
    "\n",
    "matched_books = books[books[\"isbn13\"] == isbn_num]\n",
    "print(matched_books)\n",
    "\n",
    "\n",
    "# Function to retrieve semantic recommendations\n",
    "def retrieve_semantic_recommendations(query: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    # Step 1: Semantic search in Chroma DB\n",
    "    recs = db_books.similarity_search(query, k=50)  # search top 50 similar docs\n",
    "\n",
    "    # Step 2: Extract ISBNs from the search results\n",
    "    books_list = []\n",
    "    for doc in recs[8:]:  # skip first 8 if needed\n",
    "        try:\n",
    "            isbn_str = doc.page_content.strip().split()[0]\n",
    "            isbn_num = int(''.join(filter(str.isdigit, isbn_str)))\n",
    "            books_list.append(isbn_num)\n",
    "        except:\n",
    "            continue  # skip if conversion fails\n",
    "\n",
    "    # Step 3: Return matching books from CSV\n",
    "    return books[books[\"isbn13\"].isin(books_list)].head(top_k)\n",
    "\n",
    "\n",
    "recommendations = retrieve_semantic_recommendations(\"A book to teach children about animals\")\n",
    "print(recommendations)\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Assuming you already have your documents split into 'documents'\n",
    "# or you can use your texts column from the CSV\n",
    "texts = books[\"tagged_description\"].tolist()\n",
    "\n",
    "# Build/load Chroma DB\n",
    "db_books = Chroma.from_texts(texts, embeddings, persist_directory=\"db_books\")\n",
    "\n",
    "# Optional: persist the database (Chroma auto-persists in newer versions)\n",
    "db_books.persist()\n",
    "\n",
    "recommendations = retrieve_semantic_recommendations(\"A book to teach children about animals\")\n",
    "print(recommendations)\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "def retrieve_semantic_recommendations(query: str, db: Chroma, books_df: pd.DataFrame, top_k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform semantic search on a book vector database and return top matching books.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): User query string.\n",
    "        db (Chroma): Chroma vector database containing book embeddings.\n",
    "        books_df (pd.DataFrame): Original book dataset with ISBNs and metadata.\n",
    "        top_k (int): Number of recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Top-k recommended books with full metadata.\n",
    "    \"\"\"\n",
    "    # Step 1: Semantic search\n",
    "    recs = db.similarity_search(query, k=50)  # Search top 50 similar documents\n",
    "\n",
    "    # Step 2: Extract ISBNs safely\n",
    "    isbn_list = []\n",
    "    for doc in recs:\n",
    "        text = doc.page_content.strip()\n",
    "        # Attempt to extract numeric ISBN from the start of the text\n",
    "        parts = text.split()\n",
    "        if parts:\n",
    "            isbn_str = ''.join(filter(str.isdigit, parts[0]))\n",
    "            if isbn_str.isdigit():\n",
    "                isbn_list.append(int(isbn_str))\n",
    "\n",
    "    # Step 3: Filter books by ISBN and return top_k\n",
    "    recommended_books = books_df[books_df[\"isbn13\"].isin(isbn_list)].head(top_k)\n",
    "\n",
    "    return recommended_books\n",
    "\n",
    "\n",
    "# Make sure db_books is your Chroma vector store and books is your DataFrame\n",
    "query = \"A book to teach children about animals\"\n",
    "recommendations = retrieve_semantic_recommendations(query, db_books, books, top_k=10)\n",
    "print(recommendations)\n",
    "\n",
    "query = \"Stories about friendship and adventure\"\n",
    "recommendations = retrieve_semantic_recommendations(query, db_books, books, top_k=5)\n",
    "print(recommendations)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          isbn13      isbn10                title  \\\n",
      "0  9780002005883  0002005883               Gilead   \n",
      "1  9780002261982  0002261987         Spider's Web   \n",
      "2  9780006178736  0006178731       Rage of angels   \n",
      "3  9780006280897  0006280897       The Four Loves   \n",
      "4  9780006280934  0006280935  The Problem of Pain   \n",
      "\n",
      "                           authors                     categories  \\\n",
      "0               Marilynne Robinson                        Fiction   \n",
      "1  Charles Osborne;Agatha Christie  Detective and mystery stories   \n",
      "2                   Sidney Sheldon                        Fiction   \n",
      "3              Clive Staples Lewis                 Christian life   \n",
      "4              Clive Staples Lewis                 Christian life   \n",
      "\n",
      "                                           thumbnail  \\\n",
      "0  http://books.google.com/books/content?id=KQZCP...   \n",
      "1  http://books.google.com/books/content?id=gA5GP...   \n",
      "2  http://books.google.com/books/content?id=FKo2T...   \n",
      "3  http://books.google.com/books/content?id=XhQ5X...   \n",
      "4  http://books.google.com/books/content?id=Kk-uV...   \n",
      "\n",
      "                                         description  published_year  \\\n",
      "0  A NOVEL THAT READERS and critics have been eag...          2004.0   \n",
      "1  A new 'Christie for Christmas' -- a full-lengt...          2000.0   \n",
      "2  A memorable, mesmerizing heroine Jennifer -- b...          1993.0   \n",
      "3  Lewis' work on the nature of love divides love...          2002.0   \n",
      "4  \"In The Problem of Pain, C.S. Lewis, one of th...          2002.0   \n",
      "\n",
      "   average_rating  num_pages  ratings_count      title_and_subtitle  \\\n",
      "0            3.85      247.0          361.0                  Gilead   \n",
      "1            3.83      241.0         5164.0  Spider's Web + A Novel   \n",
      "2            3.93      512.0        29532.0          Rage of angels   \n",
      "3            4.15      170.0        33684.0          The Four Loves   \n",
      "4            4.09      176.0        37569.0     The Problem of Pain   \n",
      "\n",
      "                                  tagged_description  \n",
      "0  9780002005883 A NOVEL THAT READERS and critics...  \n",
      "1  9780002261982 A new 'Christie for Christmas' -...  \n",
      "2  9780006178736 A memorable, mesmerizing heroine...  \n",
      "3  9780006280897 Lewis' work on the nature of lov...  \n",
      "4  9780006280934 \"In The Problem of Pain, C.S. Le...  \n",
      "Total chunks created: 5197\n",
      "[Document(metadata={}, page_content='This is a novel about love.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (9.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\msi\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "[Document(metadata={}, page_content='This is a novel about love.'), Document(metadata={}, page_content='This is a novel about love.'), Document(metadata={}, page_content='This is a book about AI.'), Document(metadata={}, page_content='This is a book about AI.')]\n",
      "This is a novel about love.\n",
      "Cannot convert ISBN to int: This is a novel about love.\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
